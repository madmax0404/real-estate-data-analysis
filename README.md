# Real Estate Analytics & ML Portfolio

This repository documents my production-scale machine learning and geospatial analytics work on the Seoul metropolitan real estate market. It combines raw public datasets, reproducible feature engineering, procurement automation, and model evaluation assets that support acquisition, valuation, and decision workflows across residential and land portfolios. The codebase spans 931 Jupyter notebooks plus lightweight Python orchestration so you can inspect my technical depth end-to-end.

## Portfolio Highlights
- 931 versioned notebooks covering data acquisition, feature engineering, modeling, automation, and executive reporting.
- 20+ public datasets from 국토교통부, 한국부동산원, 서울시 열린데이터 광장, etc., consolidated into reusable feature stores with documented quality checks.
- Gradient boosting pipelines (LightGBM, CatBoost) with GroupKFold validation, hyperparameter sweeps, and SHAP explainability baked into the workflow.
- Automation utilities for procurement scoring, coordinate enrichment, and high-impact geospatial storytelling (folium, pptk, vispy, open3d).

## Core Workstreams
- **Residential market intelligence** (`notebooks/analysis_residential/`, 471 notebooks): trend benchmarking, micro-market anomaly surfacing, acquisition prioritization.
- **Land valuation** (`notebooks/analysis_land/`, 134 notebooks): zoning joins, redevelopment scenarios, 토지특성정보 feature blends.
- **Data preparation & feature stores** (`notebooks/data_preparation/`, 140 notebooks): address normalization, coordinate stitching, categoricals, reusable tables under `data/processed/`.
- **Training & modeling** (`notebooks/training/`, 79 notebooks): LightGBM/CatBoost experiments with KFold + GroupKFold validation and feature selection (VIF, SHAP dropouts).
- **Procurement automation** (`notebooks/procurement/`): Selenium-based scrapers, scoring heuristics, Excel exports for SH & LH portfolio opportunities.
- **Maps & visualization** (`notebooks/maps/`): folium dashboards, pptk/vispy point clouds, executive-ready storytelling assets.

## Flagship Assets
- [Feature engineering pipeline](notebooks/data_preparation/%EB%A9%94%EC%9D%B8_%EB%8D%B0%EC%9D%B4%ED%84%B0_20060101_20210416_ver_3%20-%20%EC%A2%8C%ED%91%9C%20%EC%A0%81%EC%9A%A9.ipynb) — orchestrates cadastral, coordinate, and zoning merges into a modeling-ready table.
- [Model tuning & evaluation](notebooks/training/Training%2025%20-%20training%20data%20ver%2016%20-%20KFold%20shuffle%20-%20l2%20tuning.ipynb) — LightGBM pipeline with GroupKFold validation, hyperparameter search, and SHAP diagnostics.
- [District-level pricing analysis](notebooks/analysis_residential/%EC%A7%91%EA%B0%92%EB%B6%84%EC%84%9D%20-%20%EC%86%A1%ED%8C%8C%EA%B5%AC%20%EC%84%9D%EC%B4%8C%EB%8F%99%20%EC%97%B0%EB%A6%BD%EB%8B%A4%EC%84%B8%EB%8C%80%2020210513.ipynb) — combines transactional, structural, and spatial features for 송파구 scenario planning.
- [Selenium procurement automation](notebooks/procurement/%EC%9D%BC%EA%B4%84%EA%B5%AC%EB%A7%A4%20%EC%B0%BE%EA%B8%B0%20-%2010%ED%98%B8%20%EC%9D%B4%EC%83%81%20(%EC%95%84%ED%8C%8C%ED%8A%B8%20%ED%8F%AC%ED%95%A8)%20-%20SH%20%26%20LH%20%EC%B0%BE%EA%B8%B0%20-%20selenium.ipynb) — scrapes bulk listings, applies proprietary scoring, and exports ranked leads.

## Visual Portfolio
<p align="center">
  <img src="docs/assets/ScreenCapture_2021-11-08-13-47-51.png" alt="Seoul building footprint point cloud" width="70%">
</p>
<p align="center">
  <em>Seoul-wide building footprint map synthesized from cadastral and coordinate datasets.</em>
</p>
<p align="center">
  <img src="docs/assets/ScreenCapture_2021-11-08-13-49-02.png" alt="3D valuation intensity heatmap" width="70%">
</p>
<p align="center">
  <em>3D valuation intensity visualization highlighting vertical price gradients across districts.</em>
</p>

## Data Footprint & Git Hygiene

Large datasets, model artifacts, and secrets are intentionally excluded from version control via the expanded `.gitignore`. Hydrate them locally as needed:

| Path | Purpose | Git status | How to hydrate |
|------|---------|------------|----------------|
| `data/raw/` | Source acquisitions from 국토교통부, 한국부동산원, 서울시 열린데이터 광장, etc. | git-ignored | Download from the respective portals and mirror the existing Korean folder names (e.g., `한국부동산원`, `좌표데이터`). |
| `data/processed/` | Engineered tables, KPIs, Excel deliverables | git-ignored | Rebuild by running the pipelines in `notebooks/data_preparation/` and downstream analysis notebooks. |
| `artifacts/models/` | LightGBM/CatBoost boosters, training logs, CV metrics | git-ignored | Generated by notebooks under `notebooks/training/` and `projects/real_estate_ml/`. |
| `config/` | API keys & secrets (e.g., 주소 검색, Kakao geocoder) | git-ignored | Create locally and insert your credentials following the variable names referenced in the notebooks. |
| `resources/fonts/` | Nanum font files for Korean map labeling | `.ttf` files are git-ignored | Install Nanum fonts system-wide or place the `.ttf` files locally (kept out of Git for licensing). |

> **Note 1**: All datasets used were publicly available through Korean open government portals.<br>
> **Note 2**: Standalone `*.csv`, `*.txt`, and `*.xlsx` artifacts are ignored globally. Exported results ship as documentation snapshots instead; regenerate them from notebooks when needed.

## Environment Setup
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install -e .[visual3d,domain]  # drop extras if 3D or domain APIs are unavailable
jupyter lab
```
1. (Optional) Register a kernelspec: `python -m ipykernel install --user --name real-estate-venv`.
2. Run `python main.py` for a smoke test that validates imports and folder expectations.

## Reproducing the Pipeline
1. Populate `data/raw/` with the latest public datasets; maintain the original Korean directory names for joins to resolve automatically.
2. Execute notebooks in this order: `notebooks/data_preparation/` → `notebooks/training/` → `notebooks/analysis_*` → `notebooks/maps/`.
3. Export stakeholder deliverables from `data/processed/` (Excel dashboards, feature maps) or regenerate visuals directly from notebooks.
4. Capture experiments inside `artifacts/models/Training_*`; the directory stays local but notebooks log metrics for reproducibility.

## Modeling & Experimentation Stack
- Blend gradient boosting (LightGBM, CatBoost) with linear/tree baselines; leverage `probatus` and `statsmodels` for calibration and inference checks.
- Group-aware cross-validation (`GroupKFold`, `KFold`) coupled with Huber/RMSE objectives to respect building-level leakage constraints.
- SHAP-based model explainability (`Training 28–45` series) guiding feature selection, feature importance narratives, and top-N driver reports.
- Feature engineering pipelines under `projects/real_estate_ml/` use Dask, multiprocessing, and targeted Numba kernels to scale cadastral joins.

## Automation & Decision Support
- Selenium procurement crawlers flag SH/LH bulk purchase opportunities, score portfolios, and export ranked lists for due diligence.
- Coordinate enrichment utilities (`좌표 데이터 준비`, `polylabel 활용`) compute centroids, frontage metrics, and shapefiles for GIS/3D viewers.
- Reporting outputs (`data/processed/지역별_땅값_일괄매매_평균.xlsx`, etc.) tie model insights back to business stakeholders—even though the files are git-ignored, the generating notebooks are available.

## Repository Layout
```
├─ notebooks/
│  ├─ analysis_residential/
│  ├─ analysis_land/
│  ├─ data_preparation/
│  ├─ training/
│  ├─ procurement/
│  ├─ maps/
│  └─ misc/
├─ projects/real_estate_ml/      # End-to-end feature engineering & modeling pipelines
├─ docs/                         # Screenshots, references, contributor guides
├─ data/                         # Local raw/processed datasets (git-ignored; create as needed)
├─ artifacts/                    # Local model artifacts (git-ignored)
├─ config/                       # Local API keys & secrets (git-ignored)
├─ resources/                    # Shared static assets (fonts git-ignored)
├─ main.py                       # Thin entry point for scripted workflows
├─ requirements.txt
└─ pyproject.toml
```

## Impact & Outcomes
- Ranked SH/LH bulk acquisition targets, highlighting mixed-asset buildings and exporting prioritized lead lists for investment teams.
- Built reusable feature stores and evaluation tracks that shorten experiment setup time and guard against data drift.
- Produced geospatial storytelling assets (folium dashboards, pptk/vispy scenes, 3D point clouds) used in executive briefings and investor updates.

## Roadmap
- Parameterize recurring pipelines with Papermill/nbclient for scheduled refreshes.
- Harden procurement scrapers with retry/backoff logic and dataset diffing for change detection.
- Publish a lightweight API or Streamlit app to expose top acquisition candidates and scenario toggles.
